{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score\n",
    "from functools import reduce\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning / Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('D:/EYChallenge/data_train.csv')\n",
    "test = pd.read_csv('D:/EYChallenge/data_test.csv')\n",
    "train = train.drop(['Unnamed: 0'],axis=1)\n",
    "test = test.drop(['Unnamed: 0'],axis=1)\n",
    "######################################################################################################\n",
    "tr = test.loc[test.x_exit.isnull()]\n",
    "\n",
    "def conv(time_1):   \n",
    "    sec1 = 3600*int(time_1[0:2]) + 60*int(time_1[3:5]) + int(time_1[6:8])                                                                                                                                \n",
    "    return (sec1)\n",
    "def distance(x1,x2,y1,y2):\n",
    "    dist = np.sqrt( ((x2-x1)**2) + ((y2-y1)**2) )\n",
    "######################################################################################################\n",
    "#DISTANCE CENTRE\n",
    "\n",
    "def distanceCentre(x,y):\n",
    "    dx = max(0,abs(x_centre-x)-10000)\n",
    "    dy = max(0,abs(y_centre-y)-30000)\n",
    "    return (dx**2+dy**2)\n",
    "distanceCentre = np.vectorize(distanceCentre)\n",
    "x_centre = 3760901.5068\n",
    "y_centre = -19238905.6133\n",
    "\n",
    "train['distCenter_entry']=distanceCentre(train.x_entry,train.y_entry)\n",
    "test['distCenter_entry']=distanceCentre(test.x_entry,test.y_entry)\n",
    "\n",
    "train['distCenter_exit']=distanceCentre(train.x_exit,train.y_exit)\n",
    "test['distCenter_exit']=distanceCentre(test.x_exit,test.y_exit)\n",
    "\n",
    "train['Ecart_dist']=train['distCenter_exit']-train['distCenter_entry']\n",
    "test['Ecart_dist']=test['distCenter_exit']-test['distCenter_entry']\n",
    "######################################################################################################\n",
    "#ANGLES\n",
    "\n",
    "u = np.array([train.x_exit,train.y_exit])- np.array([train.x_entry,train.y_entry])\n",
    "v = np.array([x_centre-train.x_entry,y_centre-train.y_entry])\n",
    "\n",
    "angle_train = []\n",
    "for i in range(u.shape[1]):\n",
    "    scalar = (np.dot([u[0][i],u[1][i]],[v[0][i],v[1][i]]))\n",
    "    u_norm = (np.linalg.norm([u[0][i],u[1][i]]))\n",
    "    v_norm = (np.linalg.norm([v[0][i],v[1][i]]))\n",
    "    angle_train.append(math.degrees(math.acos(scalar/(u_norm*v_norm))))\n",
    "    \n",
    "u = np.array([test.x_exit,test.y_exit])-np.array([test.x_entry,test.y_entry])\n",
    "v = np.array([x_centre-test.x_entry,y_centre-test.y_entry])\n",
    "\n",
    "angle_test = []\n",
    "for i in range(u.shape[1]):\n",
    "    scalar = (np.dot([u[0][i],u[1][i]],[v[0][i],v[1][i]]))\n",
    "    u_norm = (np.linalg.norm([u[0][i],u[1][i]]))\n",
    "    v_norm = (np.linalg.norm([v[0][i],v[1][i]]))\n",
    "    angle_test.append(math.degrees(math.acos(scalar/(u_norm*v_norm))))\n",
    "\n",
    "train['angle']=angle_train\n",
    "test['angle']=angle_test\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "train.vmean[train.vmean<0]=np.NaN\n",
    "train.vmin[train.vmin<0]=np.NaN\n",
    "train.vmax[train.vmax<0]=np.NaN\n",
    "\n",
    "test.vmean[test.vmean<0]=np.NaN\n",
    "test.vmin[test.vmin<0]=np.NaN\n",
    "test.vmax[test.vmax<0]=np.NaN\n",
    "\n",
    "#incohÃ©rences\n",
    "train['vmean'][(train.time_entry==train.time_exit)] = 0\n",
    "train['vmin'][(train.time_entry==train.time_exit)] = 0\n",
    "train['vmax'][(train.time_entry==train.time_exit)] = 0\n",
    "\n",
    "test['vmean'][(test.time_entry==test.time_exit)] = 0\n",
    "test['vmin'][(test.time_entry==test.time_exit)] = 0\n",
    "test['vmax'][(test.time_entry==test.time_exit)] = 0\n",
    "\n",
    "train['vmax'][(train.x_entry==train.x_exit)&(train.y_entry==train.y_exit)] = 0\n",
    "train['vmin'][(train.x_entry==train.x_exit)&(train.y_entry==train.y_exit)] = 0\n",
    "train['vmean'][(train.x_entry==train.x_exit)&(train.y_entry==train.y_exit)] = 0\n",
    "\n",
    "test['vmax'][(test.x_entry==test.x_exit)&(test.y_entry==test.y_exit)] = 0\n",
    "test['vmin'][(test.x_entry==test.x_exit)&(test.y_entry==test.y_exit)] = 0\n",
    "test['vmean'][(test.x_entry==test.x_exit)&(test.y_entry==test.y_exit)] = 0\n",
    "\n",
    "#outliers\n",
    "train.vmean = train.vmean.loc[(train.vmean<90)]\n",
    "train.vmax = train.vmean.loc[(train.vmax<90)]\n",
    "train.vmin = train.vmean.loc[(train.vmin<90)]\n",
    "\n",
    "######################################################################################################\n",
    "train['vmax']=train['vmax'].replace( np.NaN , train.vmax.mean())\n",
    "train['vmin']=train['vmin'].replace( np.NaN , train.vmin.mean())\n",
    "train['vmean']=train['vmean'].replace( np.NaN , train.vmean.mean())\n",
    "test['vmax']=test['vmax'].replace( np.NaN , test.vmax.mean())\n",
    "test['vmin']=test['vmin'].replace( np.NaN , test.vmin.mean())\n",
    "test['vmean']=test['vmean'].replace( np.NaN , test.vmean.mean())\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "vec_conv =  np.vectorize(conv)\n",
    "train.time_entry=vec_conv(train.time_entry)\n",
    "train.time_exit=vec_conv(train.time_exit)\n",
    "\n",
    "test.time_entry=vec_conv(test.time_entry)\n",
    "test.time_exit=vec_conv(test.time_exit)\n",
    "\n",
    "def distance(x1,x2,y1,y2):\n",
    "    dist = np.sqrt( ((x2-x1)**2) + ((y2-y1)**2) )\n",
    "distance = np.vectorize(distance)\n",
    "def ecartTemps(t1,t2):\n",
    "    ecart= t2-t1\n",
    "    return ecart\n",
    "ecartTemps = np.vectorize(ecartTemps)\n",
    "train['duree']=ecartTemps(train.time_entry,train.time_exit)\n",
    "test['duree']=ecartTemps(test.time_entry,test.time_exit)\n",
    "\n",
    "train['city_center'] = 0\n",
    "train['city_center'][(train.x_exit>=3750901.5068) & (train.x_exit<=3770901.5068)&(train.y_exit>=(-19268905.6133)) & (train.y_exit<=(-19208905.6133))]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Moments\n",
    "# train['nuit']= 0\n",
    "# train['nuit'][(train.time_entry<28800)]=1\n",
    "# train['matin']= 0\n",
    "# train['matin'][(train.time_entry<43200)&(train.time_entry>28800)]=1\n",
    "# train['aprem']= 0\n",
    "# train['aprem'][(train.time_entry>=43200)]=1\n",
    "\n",
    "# test['nuit']= 0\n",
    "# test['nuit'][(test.time_entry<28800)]=1\n",
    "# test['matin']= 0\n",
    "# test['matin'][(test.time_entry<43200)&(test.time_entry>28800)]=1\n",
    "# test['aprem']= 0\n",
    "# test['aprem'][(test.time_entry>=43200)]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['time_entry','time_exit'],axis=1)\n",
    "test = test.drop(['time_entry','time_exit'],axis=1)\n",
    "\n",
    "y =  train.groupby('hash')['city_center'].apply(list)\n",
    "for i in range (y.shape[0]):\n",
    "    y[i]=y[i][-1]\n",
    "Y = pd.DataFrame(y.values)\n",
    "\n",
    "train = train.drop(['trajectory_id','city_center'],axis=1)\n",
    "test = test.drop(['trajectory_id'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert trajectories on one line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformOnLine(df):\n",
    "    lesdataframes=[]\n",
    "\n",
    "    for vincent in df.columns[1:]:\n",
    "        print(vincent)\n",
    "        k=df\n",
    "        k = k.groupby('hash')[vincent].apply(list)\n",
    "\n",
    "        if (vincent == 'distCenter_exit') | (vincent == 'x_exit') | (vincent == 'y_exit'):# | (vincent == 'Ecart_dist'):\n",
    "            for i in range (k.shape[0]):\n",
    "                k[i].reverse()\n",
    "                k[i]=k[i][1:]      #x_exit y_exit\n",
    "                \n",
    "        elif (vincent == 'angle'):\n",
    "            for i in range (k.shape[0]):\n",
    "                try:\n",
    "                    k[i]=k[i][-2]\n",
    "                except(IndexError):\n",
    "                    #k[i]=-1\n",
    "                    k[i]=np.NaN\n",
    "                    \n",
    "        elif (vincent == 'Ecart_dist'):\n",
    "            for i in range (k.shape[0]):\n",
    "                k[i]=(np.cumsum(np.array(k[i][:-1]))).tolist()\n",
    "                    \n",
    "#                  k[i].reverse()\n",
    "#                  k[i]=np.nanmean(np.array(k[i][1:]))\n",
    "\n",
    "        elif (vincent == 'vmin') | (vincent == 'vmax') | (vincent == 'vmean'):\n",
    "            for i in range (k.shape[0]):\n",
    "                k[i].reverse()\n",
    "                m=np.nanmean(np.array(k[i]))\n",
    "#                 M=max(k[i])\n",
    "#                 mi=min(k[i])\n",
    "                k[i]=[m]\n",
    "#                 last=k[i][0]\n",
    "#                 if (last!=0):\n",
    "#                     if (len(k[i])>1):\n",
    "#                         k[i][0]=np.nanmean(np.array(k[i][1:]))\n",
    "#                     else:\n",
    "#                         k[i][0]=-1\n",
    "                \n",
    "        else :\n",
    "            for i in range (k.shape[0]):\n",
    "                k[i].reverse()\n",
    "\n",
    "\n",
    "        k=pd.DataFrame(k)\n",
    "\n",
    "\n",
    "        # expand df.time_entry into its own dataframe\n",
    "        gouz = k[vincent].apply(pd.Series)\n",
    "\n",
    "        # rename each variable is time_entry\n",
    "        gouz = gouz.rename(columns = lambda x : vincent + str(x))\n",
    "        \n",
    "        if(vincent == 'distCenter_exit') | (vincent == 'distCenter_entry') | (vincent == 'Ecart_dist'):\n",
    "            gouz = gouz.fillna(math.inf)\n",
    "        elif(vincent!='angle'):\n",
    "            gouz = gouz.fillna(0)\n",
    "\n",
    "#         if(vincent == 'Ecart_dist'):\n",
    "#             gouz = gouz.fillna(0)\n",
    "#         else:\n",
    "#             gouz = gouz.fillna(-1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        lesdataframes.append(gouz)\n",
    "\n",
    "    r = pd.concat(lesdataframes,axis=1)\n",
    "    r = r.reset_index()\n",
    "    r = r.drop(['hash'],axis=1)\n",
    "    #r.columns = np.arange(len(r.columns))\n",
    "    \n",
    "#     sup = r.vmax0.loc[r.vmax0>=0]\n",
    "#     r.vmax0.loc[r.vmax0<0] = sup.mean()\n",
    "#     sup = r.vmin0.loc[r.vmin0>=0]\n",
    "#     r.vmin0.loc[r.vmin0<0] = sup.mean()\n",
    "#     sup = r.vmean0.loc[r.vmean0>=0]\n",
    "#     r.vmean0.loc[r.vmean0<0] = sup.mean()\n",
    "    \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = transformOnLine(train)\n",
    "X_test = transformOnLine(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X = X\n",
    "y = Y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find best hyperparameters (with stratified kfold cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #GRID SEARCH\n",
    "# X = X\n",
    "# y = Y.astype(int)\n",
    "# param_grid = {\n",
    "#     'n_estimators':[190,200,250], \n",
    "#     'max_depth' : [9],\n",
    "#     'learning_rate' : [0.09,0.1],\n",
    "#     'num_leaves':[70,75],\n",
    "#     'min_data_in_leaf' : [300],\n",
    "#     'lambda_l2' : [0.6,0.7],\n",
    "#     'colsample_bytree': [0.5,0.6],\n",
    "#     'min_child_samples':[20]\n",
    "# }\n",
    "\n",
    "# for i in range(10):\n",
    "#     kf = StratifiedKFold(n_splits=5,random_state=1,shuffle=True)\n",
    "#     GridS = GridSearchCV(lgb.LGBMClassifier(n_jobs=-1),param_grid, cv=kf.split(X, y),verbose=3,n_jobs=-1, scoring= 'f1',iid=True)\n",
    "#     GridS.fit(X,y)\n",
    "#     print(GridS.best_params_)\n",
    "#     print(GridS.best_estimator_)\n",
    "# pickle.dump(GridS.best_estimator_, open('LGBMTunedV16', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the best threshold for probabilities and evaluate the score of the models (with stratified kfold cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findBestMeanEnsembling(listModel,nb_splits,shuffling):\n",
    "    listTreshold = []\n",
    "    listScore = []\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=nb_splits, shuffle=shuffling)    \n",
    "    y = Y.astype(int)\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        x_train, x_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "        \n",
    "        listPrediction = []\n",
    "        for each in listModel:\n",
    "            each.fit(x_train,y_train)\n",
    "            prediction = each.predict_proba(x_test)\n",
    "            prediction = pd.DataFrame(prediction) #proba\n",
    "            prediction = prediction[1] #proba    \n",
    "            listPrediction.append(prediction)\n",
    "            \n",
    "        ensemble_pred = pd.concat(listPrediction, ignore_index=True,axis=1)\n",
    "        ensemble_pred['probaMean'] = ensemble_pred.mean(axis='columns',numeric_only=True)\n",
    "        \n",
    "        treshold = np.arange(0.42,0.52,0.001)\n",
    "        listDic = {}\n",
    "        for each in treshold:\n",
    "            ensemble_pred['target'] = np.where(ensemble_pred['probaMean'] > each, 1,0)\n",
    "            ensemble_pred['target']= ensemble_pred['target'].astype(int)\n",
    "            score = f1_score(y_test.astype(int),ensemble_pred['target'])\n",
    "            listDic[each]=score\n",
    "\n",
    "        listKeyValue = []    \n",
    "        listKeyValue.append(max(listDic.items(), key=lambda x:x[1]))\n",
    "\n",
    "        listTreshold.append([item[0] for item in listKeyValue])\n",
    "        listScore.append([item[1] for item in listKeyValue])\n",
    "\n",
    "    print(np.mean(listTreshold))\n",
    "    print(np.mean(listScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findOptimalEnsemble(listModel, ensemble,nb_splits,shuffling):\n",
    "    ensembleClassifier = ensemble\n",
    "    skf = StratifiedKFold(n_splits=nb_splits,shuffle=shuffling)\n",
    "    y = Y.astype(int)\n",
    "    listTreshold = []\n",
    "    listScore = []\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        x_train, x_test = X.loc[train_index], X.loc[test_index]\n",
    "        y_train, y_test = y.loc[train_index], y.loc[test_index]\n",
    "\n",
    "        listPrediction = []\n",
    "        for each in listModel:\n",
    "            each.fit(x_train,y_train)\n",
    "            prediction = each.predict_proba(x_test)\n",
    "            prediction = pd.DataFrame(prediction) #proba\n",
    "            prediction = prediction[1] #proba    \n",
    "            listPrediction.append(prediction)\n",
    "\n",
    "        ensemble_pred = pd.concat(listPrediction, ignore_index=True,axis=1)\n",
    "        \n",
    "        xV_train, xV_test, yV_train, yV_test = train_test_split(ensemble_pred, y_test, test_size=0.3,stratify=y_test, random_state=1)\n",
    "        ensembleClassifier.fit(xV_train,yV_train)\n",
    "        ensemblePredict = ensembleClassifier.predict_proba(xV_test)\n",
    "        ensemblePredict = pd.DataFrame(ensemblePredict)\n",
    "\n",
    "        treshold =np.arange(0.3,0.5,0.001)\n",
    "        listDic = {}\n",
    "        for each in treshold:\n",
    "            newPred = np.where(ensemblePredict[1] > each, 1,0)\n",
    "            newPred = newPred.astype(int)\n",
    "            score = f1_score(yV_test.astype(int),newPred)\n",
    "            listDic[each]=score\n",
    "\n",
    "        listKeyValue = []    \n",
    "        listKeyValue.append(max(listDic.items(), key=lambda x:x[1]))\n",
    "        listTreshold.append([item[0] for item in listKeyValue])\n",
    "        listScore.append([item[1] for item in listKeyValue])\n",
    "\n",
    "    print(np.mean(listTreshold))\n",
    "    print(np.mean(listScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1 = pickle.load(open('LGBMTunedV12','rb'))\n",
    "model1 = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=0.7,\n",
    "        importance_type='split', lambda_l2=0.5, learning_rate=0.08,\n",
    "        max_depth=10, min_child_samples=20, min_child_weight=0.001,\n",
    "        min_data_in_leaf=300, min_split_gain=0.0, n_estimators=200,\n",
    "        n_jobs=-1, num_leaves=80, objective=None, random_state=None,\n",
    "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
    "        subsample_for_bin=200000, subsample_freq=0)\n",
    "#model2 = pickle.load(open('XGB_tuned_01','rb'))\n",
    "model2 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "       colsample_bytree=0.8, gamma=0, learning_rate=0.02, max_delta_step=0,\n",
    "       max_depth=12, min_child_weight=0.4, missing=nan, n_estimators=230,\n",
    "       n_jobs=-1, nthread=None, objective='binary:logistic',\n",
    "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
    "       seed=None, silent=True, subsample=0.8)\n",
    "model3 = CatBoostClassifier(n_estimators=3000,task_type='GPU',eval_metric='F1',silent=True)\n",
    "\n",
    "listModel = [model1,model3,model2]\n",
    "for i in range(5):\n",
    "    findBestMeanEnsembling(listModel,5,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train models individually on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelToTrain = CatBoostClassifier(n_estimators=3000,task_type='GPU',eval_metric='F1')\n",
    "modelToTrain.fit(X,y)\n",
    "pred = modelToTrain.predict_proba(X_test)\n",
    "pred = pd.DataFrame(pred) #proba\n",
    "pred = pred[1] #proba\n",
    "prediction = pd.DataFrame()\n",
    "trajectory_id = tr['trajectory_id']\n",
    "trajectory_id=trajectory_id.values\n",
    "prediction['id']=trajectory_id\n",
    "prediction['target']=pred\n",
    "prediction.to_csv('CBT3000est_proba_All9.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelToTrain = model1\n",
    "modelToTrain.fit(X,y)\n",
    "pred = modelToTrain.predict_proba(X_test)\n",
    "pred = pd.DataFrame(pred) #proba\n",
    "pred = pred[1] #proba\n",
    "prediction = pd.DataFrame()\n",
    "trajectory_id = tr['trajectory_id']\n",
    "trajectory_id=trajectory_id.values\n",
    "prediction['id']=trajectory_id\n",
    "prediction['target']=pred\n",
    "prediction.to_csv(\"LGBMTunedV12_All9_proba.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelToTrain = model2\n",
    "modelToTrain.fit(X,y)\n",
    "pred = modelToTrain.predict_proba(X_test)\n",
    "pred = pd.DataFrame(pred) #proba\n",
    "pred = pred[1] #proba\n",
    "prediction = pd.DataFrame()\n",
    "trajectory_id = tr['trajectory_id']\n",
    "trajectory_id=trajectory_id.values\n",
    "prediction['id']=trajectory_id\n",
    "prediction['target']=pred\n",
    "prediction.to_csv(\"XGB_tuned_01_All9_proba.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling probabilities with best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Probas\n",
    "A = pd.read_csv('CBT3000est_proba_All9.csv')\n",
    "B = pd.read_csv('XGB_tuned_01_All9_proba.csv')\n",
    "C = pd.read_csv('LGBMTunedV12_All9_proba.csv')\n",
    "\n",
    "dfs = [A,B,C]\n",
    "thresholds = [0.4366,0.4498,0.4476,0.4592,0.4406]\n",
    "\n",
    "ensemble_pred = reduce(lambda left,right: pd.merge(left,right,on='id'), dfs)\n",
    "ensemble_pred['probaMean'] = ensemble_pred.mean(axis='columns',numeric_only=True)\n",
    "ensemble_pred = ensemble_pred[['id','probaMean']]\n",
    "ensemble_pred['probaMean'] = np.where(ensemble_pred['probaMean'] >np.mean(thresholds), 1,0)\n",
    "ensemble_pred['probaMean']= ensemble_pred['probaMean'].astype(int)\n",
    "ensemble_pred['target']= ensemble_pred['probaMean']\n",
    "ensemble_pred = ensemble_pred.drop(['probaMean'],axis=1)\n",
    "ensemble_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_pred.to_csv(\"LGBMV12_CBT3000est_XGBTuned_All9.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
